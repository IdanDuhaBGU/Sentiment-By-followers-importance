{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl3Vn4Pg4qJU"
      },
      "outputs": [],
      "source": [
        "!pip install textblob transformers\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import expit\n",
        "import torch\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv('/content/Merged DB.csv')\n",
        "\n",
        "# Load the toxic-bert model onto GPU (if available)\n",
        "model_name = \"unitary/toxic-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "\n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "def detect_toxicity_label(text):\n",
        "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    scores = expit(logits.cpu().numpy())[0]  # move back to CPU before converting\n",
        "    result = {label: score for label, score in zip(labels, scores)}\n",
        "    top_label = max(result, key=result.get)\n",
        "    top_score = result[top_label]\n",
        "    return f\"{round(top_score, 3)}\"\n",
        "\n",
        "# Batching settings\n",
        "batch_size = 500\n",
        "num_batches = len(df) // batch_size + 1\n",
        "\n",
        "for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = min((i + 1) * batch_size, len(df))\n",
        "    df_batch = df.iloc[start:end].copy()\n",
        "    print(f\"Processing batch {i+1}/{num_batches} (rows {start} to {end})\")\n",
        "\n",
        "    df_batch['sentiment'] = df_batch['tweet'].apply(detect_toxicity_label)\n",
        "\n",
        "    # Save batch to CSV\n",
        "    batch_file = f'/content/toxicity_output_batch_{i+1}.csv'\n",
        "    df_batch.to_csv(batch_file, index=False)\n",
        "    print(f\"Saved {batch_file}\")\n",
        "\n",
        "print(\"All batches processed!\")\n"
      ],
      "metadata": {
        "id": "i1bXdozN4wby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Find all batch CSV files (adjust the pattern if needed)\n",
        "batch_files = glob.glob('/content/toxicity_output_batch_*.csv')\n",
        "\n",
        "# Sort the files in order (optional but helps keep them ordered)\n",
        "batch_files.sort()\n",
        "\n",
        "# Read and combine all batches\n",
        "combined_df = pd.concat([pd.read_csv(f) for f in batch_files], ignore_index=True)\n",
        "\n",
        "# Save the combined file\n",
        "combined_df.to_csv('/content/toxicity_output_combined.csv', index=False)\n",
        "\n",
        "print(f\"Combined {len(batch_files)} files into 'toxicity_output_combined.csv'\")\n"
      ],
      "metadata": {
        "id": "VmiT7hM64xBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load your dataset\n",
        "dataset = pd.read_csv('/content/dataset.csv')\n",
        "\n",
        "# Clean and count words (excluding links that start with http)\n",
        "def count_words_excluding_links(text):\n",
        "    words = str(text).split()\n",
        "    filtered_words = [word for word in words if not word.lower().startswith(\"http\")]\n",
        "    return len(filtered_words)\n",
        "\n",
        "dataset['tweet_length'] = dataset['tweet'].apply(count_words_excluding_links)\n",
        "\n",
        "# Count hashtags\n",
        "dataset['hashtag_count'] = dataset['tweet'].astype(str).apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
        "\n",
        "# Preview\n",
        "print(dataset[['tweet', 'tweet_length', 'hashtag_count']].head())\n",
        "\n",
        "# Save and download\n",
        "dataset.to_csv('/content/dataset_cleaned.csv', index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/dataset_cleaned.csv')\n"
      ],
      "metadata": {
        "id": "85bzxCjc4zf3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}